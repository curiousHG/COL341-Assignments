{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "np.random.seed(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "train_data = pd.read_csv('toy_dataset/toy_dataset_train.csv',header=None)\n",
    "X_train = train_data.iloc[:, 1:].values\n",
    "X_train = X_train.astype(np.float)\n",
    "X_train = X_train / 255\n",
    "y_train = train_data.iloc[:, 0].values\n",
    "y_true= np.array(pd.get_dummies(y_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Activation_function():\n",
    "    def __init__(self, activation_function):\n",
    "        self.name = activation_function\n",
    "        self.function = self.activation_function()\n",
    "        self.derivative = self.activation_function_derivative()\n",
    "\n",
    "    def activation_function(self):\n",
    "        if self.name == 'sigmoid':\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.name== 'relu':\n",
    "            return lambda x: np.maximum(0, x)\n",
    "        elif self.name == 'tanh':\n",
    "            return lambda x: np.tanh(x)\n",
    "        elif self.name == 'softmax':\n",
    "            return lambda x: np.exp(x - np.max(x)) / np.sum(np.exp(x- np.max(x)),axis=0)\n",
    "\n",
    "    def activation_function_derivative(self):\n",
    "        if self.name == 'sigmoid':\n",
    "            return lambda x: x * (1 - x)\n",
    "            # return lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
    "        elif self.name == 'relu':\n",
    "            return lambda x: 1. * (x > 0)\n",
    "        elif self.name == 'tanh':\n",
    "            return lambda x: 1 - np.power(x, 2)\n",
    "        elif self.name == 'softmax':\n",
    "            # return lambda x: softmax(x,axis = 0)*(1-softmax(x,axis = 0))\n",
    "            return lambda x: x*(1-x)\n",
    "        \n",
    "        \n",
    "\n",
    "def loss_function_CE(y_true, y_pred, derivative):\n",
    "    if not derivative:\n",
    "        return -np.sum(y_true * np.log(y_pred))/y_true.shape[1]\n",
    "    else:\n",
    "        return (y_pred-y_true)/y_true.shape[1]\n",
    "\n",
    "\n",
    "def loss_function_MSE(y_true, y_pred, derivative=False):\n",
    "    if not derivative:\n",
    "        return np.sum((y_true - y_pred) ** 2) / 2\n",
    "    else:\n",
    "        print(y_pred.shape, y_true.shape)\n",
    "        temp = np.sum(2*(y_pred - y_true)*y_pred,axis = 0)\n",
    "        f = 2*(y_pred - y_true)*y_pred*(1-y_pred) - y_pred*temp\n",
    "        print(temp.shape, f.shape)\n",
    "        return f/y_true.shape[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "a = np.arange(8).reshape(2,4)\n",
    "b = np.arange(8).reshape(2,4)*0.2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "c = np.sum(2*(b - a)*b,axis = 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "b"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0. , 0.2, 0.4, 0.6],\n",
       "       [0.8, 1. , 1.2, 1.4]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "c"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ -5.12,  -8.32, -12.8 , -18.56])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "b*c"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ -0.   ,  -1.664,  -5.12 , -11.136],\n",
       "       [ -4.096,  -8.32 , -15.36 , -25.984]])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "x = np.ones(y_true.T.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "loss_function_MSE(y_true.T, x,derivative=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 3000) (2, 3000)\n",
      "(3000,) (2, 3000)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.00066667, -0.00066667, -0.00066667, ..., -0.00066667,\n",
       "        -0.00066667, -0.00066667],\n",
       "       [-0.00066667, -0.00066667, -0.00066667, ..., -0.00066667,\n",
       "        -0.00066667, -0.00066667]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class Neural_Network():\n",
    "\n",
    "    def __init__(\n",
    "        self, input_size, hidden_layer_size_array, output_size, \n",
    "        activation_function, output_activation_function, loss_function\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.number_of_hidden_layers = len(hidden_layer_size_array)\n",
    "        self.hidden_layer_size_array = hidden_layer_size_array\n",
    "        self.output_size = output_size\n",
    "        self.activation_function = Activation_function(activation_function)\n",
    "        self.output_activation_function = Activation_function(output_activation_function)\n",
    "        self.weights = self.weights_initializer()\n",
    "        self.loss_function = loss_function\n",
    "        self.weights_history = []\n",
    "\n",
    "    def weights_initializer(self):\n",
    "        weights = []\n",
    "        for i in range(self.number_of_hidden_layers+1):\n",
    "            if i == 0:\n",
    "                weights.append(\n",
    "                    np.random.normal(size = (self.input_size + 1, self.hidden_layer_size_array[i]))\n",
    "                    * np.sqrt(2 / (self.input_size + self.hidden_layer_size_array[i] + 1))\n",
    "                    .astype(np.float32)\n",
    "                )\n",
    "            elif i == self.number_of_hidden_layers:\n",
    "                weights.append(\n",
    "                    np.random.normal(size = (self.hidden_layer_size_array[i - 1] + 1, self.output_size)) \n",
    "                    * np.sqrt(2 / (self.hidden_layer_size_array[i - 1] + self.output_size + 1))\n",
    "                    .astype(np.float32)\n",
    "                )\n",
    "            else:\n",
    "                weights.append(\n",
    "                    np.random.normal(size = (self.hidden_layer_size_array[i - 1] + 1, self.hidden_layer_size_array[i])) \n",
    "                    * np.sqrt(2 / (self.hidden_layer_size_array[i - 1] + self.hidden_layer_size_array[i] + 1))\n",
    "                    .astype(np.float32)\n",
    "                )\n",
    "        return weights\n",
    "    \n",
    "    def feed_forward(self, input_data):\n",
    "        # Feed forward\n",
    "        # Input layer\n",
    "        input_layer = np.array(input_data, ndmin=2).T\n",
    "        \n",
    "        # Hidden layers\n",
    "        a_s = []\n",
    "        z_s = []\n",
    "        for i in range(self.number_of_hidden_layers):\n",
    "            input_layer = np.concatenate((np.ones((1, input_layer.shape[1])), input_layer), axis=0)\n",
    "            z = np.dot(self.weights[i].T, input_layer)\n",
    "            z_s.append(z)\n",
    "            a_s_i = self.activation_function.function(z)\n",
    "            a_s.append(a_s_i)\n",
    "            input_layer = a_s[i]\n",
    "            a_s[i] = np.concatenate((np.ones((1, a_s[i].shape[1])), a_s[i]), axis=0)\n",
    "        \n",
    "        # Output layer\n",
    "        input_layer = np.concatenate((np.ones((1, input_layer.shape[1])), input_layer), axis=0)\n",
    "        z = np.dot(self.weights[-1].T, input_layer)\n",
    "        z_s.append(z)\n",
    "        a_s.append(self.output_activation_function.function(z))\n",
    "        return a_s[-1], a_s, z_s\n",
    "\n",
    "    def back_propagation2(self, input_data, y_true):\n",
    "        input_data = np.array(input_data, ndmin=2)\n",
    "        y_pred, a_s, z_s = self.feed_forward(input_data)\n",
    "\n",
    "        # Output layer\n",
    "        y_true = np.array(y_true, ndmin=2).T\n",
    "    \n",
    "        # delL_dzs = [(y_pred-y_true)/y_true.shape[1]]\n",
    "        delL_dzs = [self.loss_function(y_true, y_pred, derivative=True)]\n",
    "        delL_das = [np.dot(self.weights[-1], delL_dzs[-1])]\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(self.number_of_hidden_layers-1, -1, -1):\n",
    "            delL_dz_i = delL_das[-1]*self.activation_function.derivative(a_s[i])\n",
    "            delL_dzs.append(delL_dz_i[1:])\n",
    "            delL_das.append(np.dot(self.weights[i], delL_dzs[-1]))\n",
    "        \n",
    "        delL_dzs.reverse()\n",
    "        delL_dws = []\n",
    "        for i in range(self.number_of_hidden_layers+1):\n",
    "            if i==0:\n",
    "                input_data = np.c_[np.ones(input_data.shape[0]), input_data]\n",
    "                delL_dws.append(np.dot(input_data.T, delL_dzs[i].T))\n",
    "            else:\n",
    "                delL_dws.append(np.dot(a_s[i-1], delL_dzs[i].T))\n",
    "        \n",
    "        return delL_dws\n",
    "\n",
    "    def train(self, input_data, y_true,epochs,batch_size,learning_rate,adaptive_learning_rate = False):\n",
    "        # Training\n",
    "        iter = 0\n",
    "        for i in range(epochs):\n",
    "            for j in range(0, len(input_data), batch_size):\n",
    "                if j + batch_size > len(input_data):\n",
    "                    batch_input_data = input_data[j:]\n",
    "                    batch_y_true = y_true[j:]\n",
    "                else:\n",
    "                    batch_input_data = input_data[j:j+batch_size]\n",
    "                    batch_y_true = y_true[j:j+batch_size]\n",
    "                # del_w = self.back_propagation(batch_input_data, batch_y_true)\n",
    "                del_w = self.back_propagation2(batch_input_data, batch_y_true)\n",
    "                if adaptive_learning_rate == True:\n",
    "                    learning_rate = learning_rate * np.sqrt(1/(iter+1))\n",
    "                for k in range(len(self.weights)):\n",
    "                    self.weights[k] -= learning_rate * del_w[k]\n",
    "                iter += 1\n",
    "                if i==0 and iter == 4:\n",
    "                    self.weights_history.append(self.weights)\n",
    "            if i==4:\n",
    "                self.weights_history.append(self.weights)\n",
    "            if (i+1)%50==0:\n",
    "                print(f'Epoch:{i+1}', self.evaluate(input_data, y_true))\n",
    "    def predict(self, input_data):\n",
    "        input_data = np.array(input_data, ndmin=2)\n",
    "        y_pred, _, _ = self.feed_forward(input_data)\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, input_data, y_true):\n",
    "        y_pred = self.predict(input_data)\n",
    "        return self.loss_function(y_true.T, y_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "nn = Neural_Network(input_size = 200,\n",
    "    hidden_layer_size_array = [100, 50,20,10],\n",
    "    output_size = 2,\n",
    "    activation_function = 'relu',\n",
    "    output_activation_function = 'softmax',\n",
    "    loss_function = loss_function_CE\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "nn.train(\n",
    "    X_train, y_true,\n",
    "    epochs=5,batch_size=100,\n",
    "    learning_rate=0.001,\n",
    "    adaptive_learning_rate=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "for i in range(5):\n",
    "    k = np.load(f'checker_weights/toy_dataset/tc_3/ac_w_{i+1}.npy')\n",
    "    l = nn.weights_history[1][i]\n",
    "    print(r2_score(k,l))\n",
    "\n",
    "for i in range(5):\n",
    "    k = np.load(f'checker_weights/toy_dataset/tc_3/ac_w_{i+1}_iter.npy')\n",
    "    l = nn.weights_history[0][i]\n",
    "    print(r2_score(k,l))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-1.012659156889688\n",
      "-1.0242650918810787\n",
      "-1.1188947919149268\n",
      "-1.0986614611714791\n",
      "-4.678288841881175\n",
      "-1.0126145869763976\n",
      "-1.023988347577898\n",
      "-1.1189594361712538\n",
      "-1.0978414414647804\n",
      "-4.47475092259864\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.12 64-bit"
  },
  "interpreter": {
   "hash": "7a3d88c904243d2c3f246166597f86d1c0a39f3d97496d1fe394945d0c6d436d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}